spring.config.import=optional:file:application-secrets.properties
spring.ai.openai.chat.options.model=gpt-4o

## Max tokens does not affect the context limit, but how many tokens the model should limit itself to while generating.
# spring.ai.azure.openai.chat.options.maxTokens=100

## Instead of passing the entire list as system prompt, you could provide a tool that retrieves all jokes.
## To mimic contextlimit problems this was not used. But it will cause the same issues: bloating our context, which raises costs and can cause us to go over context limit.
## The real solution would be to provide a function that splits the file and allows the tool to retrieve a single RELEVANT random joke.
## This is intended to showcase why RAG exists.
# spring.ai.azure.openai.chat.options.functions=getJokes